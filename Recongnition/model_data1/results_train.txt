Iteration 0 | train_acc = 55.96591 | train_loss = 30.65969 | Loss1: 1.301 | Loss2: 1.50457 | Loss3: 0.84934 | Loss_concat: 27.00490 |
Iteration 1 | train_acc = 71.87500 | train_loss = 3.43312 | Loss1: 0.487 | Loss2: 0.49650 | Loss3: 0.46699 | Loss_concat: 1.98249 |
Iteration 2 | train_acc = 77.55682 | train_loss = 2.62784 | Loss1: 0.471 | Loss2: 0.48406 | Loss3: 0.44639 | Loss_concat: 1.22686 |
Iteration 3 | train_acc = 66.19318 | train_loss = 3.07428 | Loss1: 0.474 | Loss2: 0.49529 | Loss3: 0.44998 | Loss_concat: 1.65535 |
Iteration 4 | train_acc = 73.29545 | train_loss = 3.21629 | Loss1: 0.445 | Loss2: 0.47065 | Loss3: 0.43224 | Loss_concat: 1.86822 |
Iteration 5 | train_acc = 72.44318 | train_loss = 3.38781 | Loss1: 0.467 | Loss2: 0.49439 | Loss3: 0.43677 | Loss_concat: 1.98955 |
Iteration 6 | train_acc = 73.57955 | train_loss = 2.94843 | Loss1: 0.475 | Loss2: 0.49922 | Loss3: 0.45454 | Loss_concat: 1.51969 |
Iteration 7 | train_acc = 73.29545 | train_loss = 3.43267 | Loss1: 0.441 | Loss2: 0.48513 | Loss3: 0.40635 | Loss_concat: 2.10058 |
Iteration 8 | train_acc = 73.29545 | train_loss = 2.78390 | Loss1: 0.461 | Loss2: 0.48136 | Loss3: 0.43101 | Loss_concat: 1.41081 |
Iteration 9 | train_acc = 75.85227 | train_loss = 3.04690 | Loss1: 0.471 | Loss2: 0.47579 | Loss3: 0.42903 | Loss_concat: 1.67113 |
Iteration 10 | train_acc = 75.00000 | train_loss = 2.92997 | Loss1: 0.428 | Loss2: 0.45855 | Loss3: 0.38963 | Loss_concat: 1.65425 |
Iteration 11 | train_acc = 75.85227 | train_loss = 2.67626 | Loss1: 0.423 | Loss2: 0.42581 | Loss3: 0.38351 | Loss_concat: 1.44434 |
Iteration 12 | train_acc = 74.71591 | train_loss = 3.03017 | Loss1: 0.432 | Loss2: 0.44645 | Loss3: 0.38146 | Loss_concat: 1.77024 |
Iteration 13 | train_acc = 67.89773 | train_loss = 2.66509 | Loss1: 0.419 | Loss2: 0.42719 | Loss3: 0.37117 | Loss_concat: 1.44764 |
Iteration 14 | train_acc = 68.46591 | train_loss = 3.05398 | Loss1: 0.432 | Loss2: 0.44497 | Loss3: 0.39712 | Loss_concat: 1.78031 |
Iteration 15 | train_acc = 71.87500 | train_loss = 2.74873 | Loss1: 0.433 | Loss2: 0.43698 | Loss3: 0.39934 | Loss_concat: 1.47934 |
Iteration 16 | train_acc = 75.28409 | train_loss = 2.70545 | Loss1: 0.420 | Loss2: 0.43528 | Loss3: 0.36196 | Loss_concat: 1.48815 |
Iteration 17 | train_acc = 79.26136 | train_loss = 2.68601 | Loss1: 0.421 | Loss2: 0.43223 | Loss3: 0.35518 | Loss_concat: 1.47750 |
Iteration 18 | train_acc = 77.27273 | train_loss = 2.47461 | Loss1: 0.415 | Loss2: 0.43226 | Loss3: 0.36404 | Loss_concat: 1.26283 |
Iteration 19 | train_acc = 78.12500 | train_loss = 2.48848 | Loss1: 0.412 | Loss2: 0.41870 | Loss3: 0.34359 | Loss_concat: 1.31459 |
Iteration 20 | train_acc = 70.73864 | train_loss = 2.52648 | Loss1: 0.410 | Loss2: 0.41766 | Loss3: 0.35142 | Loss_concat: 1.34773 |
Iteration 21 | train_acc = 70.17045 | train_loss = 3.11417 | Loss1: 0.434 | Loss2: 0.44653 | Loss3: 0.35418 | Loss_concat: 1.87959 |
Iteration 22 | train_acc = 70.45455 | train_loss = 2.98907 | Loss1: 0.409 | Loss2: 0.40638 | Loss3: 0.34560 | Loss_concat: 1.82792 |
Iteration 23 | train_acc = 78.12500 | train_loss = 2.52251 | Loss1: 0.409 | Loss2: 0.41256 | Loss3: 0.33363 | Loss_concat: 1.36773 |
Iteration 24 | train_acc = 70.17045 | train_loss = 2.92095 | Loss1: 0.439 | Loss2: 0.44729 | Loss3: 0.36446 | Loss_concat: 1.67027 |
Iteration 25 | train_acc = 70.45455 | train_loss = 2.35472 | Loss1: 0.412 | Loss2: 0.41734 | Loss3: 0.32217 | Loss_concat: 1.20345 |
Iteration 26 | train_acc = 73.29545 | train_loss = 2.29745 | Loss1: 0.414 | Loss2: 0.42370 | Loss3: 0.33060 | Loss_concat: 1.12904 |
Iteration 27 | train_acc = 80.39773 | train_loss = 2.08670 | Loss1: 0.412 | Loss2: 0.41152 | Loss3: 0.30376 | Loss_concat: 0.95987 |
Iteration 28 | train_acc = 78.97727 | train_loss = 2.03503 | Loss1: 0.412 | Loss2: 0.41655 | Loss3: 0.32770 | Loss_concat: 0.87885 |
Iteration 29 | train_acc = 74.14773 | train_loss = 2.52319 | Loss1: 0.410 | Loss2: 0.41266 | Loss3: 0.32998 | Loss_concat: 1.37044 |
Iteration 30 | train_acc = 75.85227 | train_loss = 2.29142 | Loss1: 0.420 | Loss2: 0.41998 | Loss3: 0.31914 | Loss_concat: 1.13200 |
Iteration 31 | train_acc = 76.70455 | train_loss = 2.45054 | Loss1: 0.418 | Loss2: 0.41730 | Loss3: 0.33015 | Loss_concat: 1.28515 |
Iteration 32 | train_acc = 73.01136 | train_loss = 2.60430 | Loss1: 0.408 | Loss2: 0.41278 | Loss3: 0.33663 | Loss_concat: 1.44674 |
Iteration 33 | train_acc = 72.15909 | train_loss = 3.39598 | Loss1: 0.409 | Loss2: 0.40845 | Loss3: 0.34231 | Loss_concat: 2.23616 |
Iteration 34 | train_acc = 75.85227 | train_loss = 2.84602 | Loss1: 0.393 | Loss2: 0.39633 | Loss3: 0.35600 | Loss_concat: 1.70087 |
Iteration 35 | train_acc = 78.97727 | train_loss = 2.22383 | Loss1: 0.397 | Loss2: 0.39214 | Loss3: 0.31592 | Loss_concat: 1.11890 |
Iteration 36 | train_acc = 73.01136 | train_loss = 3.18464 | Loss1: 0.396 | Loss2: 0.38843 | Loss3: 0.32122 | Loss_concat: 2.07933 |
Iteration 37 | train_acc = 76.98864 | train_loss = 2.59567 | Loss1: 0.424 | Loss2: 0.41960 | Loss3: 0.32036 | Loss_concat: 1.43202 |
Iteration 38 | train_acc = 72.72727 | train_loss = 3.06440 | Loss1: 0.401 | Loss2: 0.41136 | Loss3: 0.28518 | Loss_concat: 1.96663 |
Iteration 39 | train_acc = 81.25000 | train_loss = 2.08853 | Loss1: 0.407 | Loss2: 0.40315 | Loss3: 0.28451 | Loss_concat: 0.99360 |
Iteration 40 | train_acc = 80.39773 | train_loss = 2.13675 | Loss1: 0.406 | Loss2: 0.40664 | Loss3: 0.28096 | Loss_concat: 1.04352 |
Iteration 41 | train_acc = 79.54545 | train_loss = 2.04067 | Loss1: 0.398 | Loss2: 0.38536 | Loss3: 0.28182 | Loss_concat: 0.97533 |
Iteration 42 | train_acc = 74.43182 | train_loss = 2.38214 | Loss1: 0.407 | Loss2: 0.40014 | Loss3: 0.28753 | Loss_concat: 1.28702 |
Iteration 43 | train_acc = 75.56818 | train_loss = 2.35912 | Loss1: 0.405 | Loss2: 0.39574 | Loss3: 0.29152 | Loss_concat: 1.26729 |
Iteration 44 | train_acc = 74.43182 | train_loss = 2.20921 | Loss1: 0.404 | Loss2: 0.40477 | Loss3: 0.27863 | Loss_concat: 1.12217 |
Iteration 45 | train_acc = 78.97727 | train_loss = 2.02099 | Loss1: 0.391 | Loss2: 0.38510 | Loss3: 0.27316 | Loss_concat: 0.97133 |
Iteration 46 | train_acc = 79.54545 | train_loss = 1.95040 | Loss1: 0.391 | Loss2: 0.37710 | Loss3: 0.27545 | Loss_concat: 0.90703 |
Iteration 47 | train_acc = 80.11364 | train_loss = 1.94332 | Loss1: 0.387 | Loss2: 0.38287 | Loss3: 0.27871 | Loss_concat: 0.89451 |
Iteration 48 | train_acc = 79.82955 | train_loss = 1.87239 | Loss1: 0.407 | Loss2: 0.39206 | Loss3: 0.30353 | Loss_concat: 0.76964 |
Iteration 49 | train_acc = 82.95455 | train_loss = 1.82961 | Loss1: 0.391 | Loss2: 0.37804 | Loss3: 0.26341 | Loss_concat: 0.79750 |
Iteration 50 | train_acc = 83.80682 | train_loss = 1.81825 | Loss1: 0.397 | Loss2: 0.38975 | Loss3: 0.26799 | Loss_concat: 0.76363 |
Iteration 51 | train_acc = 82.10227 | train_loss = 1.86632 | Loss1: 0.398 | Loss2: 0.39045 | Loss3: 0.27708 | Loss_concat: 0.80042 |
Iteration 52 | train_acc = 83.80682 | train_loss = 1.72822 | Loss1: 0.390 | Loss2: 0.38619 | Loss3: 0.25980 | Loss_concat: 0.69227 |
Iteration 53 | train_acc = 80.96591 | train_loss = 1.88548 | Loss1: 0.395 | Loss2: 0.38517 | Loss3: 0.28597 | Loss_concat: 0.81949 |
Iteration 54 | train_acc = 83.23864 | train_loss = 1.74246 | Loss1: 0.393 | Loss2: 0.37920 | Loss3: 0.25033 | Loss_concat: 0.71947 |
Iteration 55 | train_acc = 83.52273 | train_loss = 1.63766 | Loss1: 0.383 | Loss2: 0.36710 | Loss3: 0.27206 | Loss_concat: 0.61506 |
Iteration 56 | train_acc = 82.67045 | train_loss = 1.81081 | Loss1: 0.402 | Loss2: 0.38758 | Loss3: 0.28167 | Loss_concat: 0.73917 |
Iteration 57 | train_acc = 82.67045 | train_loss = 1.81699 | Loss1: 0.398 | Loss2: 0.39827 | Loss3: 0.27033 | Loss_concat: 0.75043 |
Iteration 58 | train_acc = 80.96591 | train_loss = 1.73752 | Loss1: 0.395 | Loss2: 0.38433 | Loss3: 0.26468 | Loss_concat: 0.69328 |
Iteration 59 | train_acc = 83.52273 | train_loss = 1.81716 | Loss1: 0.403 | Loss2: 0.38634 | Loss3: 0.26581 | Loss_concat: 0.76199 |
Iteration 60 | train_acc = 85.79545 | train_loss = 1.55125 | Loss1: 0.378 | Loss2: 0.36080 | Loss3: 0.24105 | Loss_concat: 0.57149 |
Iteration 61 | train_acc = 85.79545 | train_loss = 1.60508 | Loss1: 0.382 | Loss2: 0.37005 | Loss3: 0.26873 | Loss_concat: 0.58395 |
Iteration 62 | train_acc = 83.23864 | train_loss = 1.65201 | Loss1: 0.392 | Loss2: 0.37164 | Loss3: 0.25549 | Loss_concat: 0.63247 |
Iteration 63 | train_acc = 82.95455 | train_loss = 1.70628 | Loss1: 0.388 | Loss2: 0.37145 | Loss3: 0.25590 | Loss_concat: 0.69087 |
Iteration 64 | train_acc = 84.94318 | train_loss = 1.57112 | Loss1: 0.391 | Loss2: 0.37550 | Loss3: 0.25910 | Loss_concat: 0.54600 |
Iteration 65 | train_acc = 83.52273 | train_loss = 1.68348 | Loss1: 0.401 | Loss2: 0.38673 | Loss3: 0.26410 | Loss_concat: 0.63211 |
Iteration 66 | train_acc = 85.79545 | train_loss = 1.56110 | Loss1: 0.379 | Loss2: 0.35656 | Loss3: 0.23710 | Loss_concat: 0.58831 |
Iteration 67 | train_acc = 84.65909 | train_loss = 1.62449 | Loss1: 0.385 | Loss2: 0.36462 | Loss3: 0.25403 | Loss_concat: 0.62091 |
Iteration 68 | train_acc = 84.94318 | train_loss = 1.50318 | Loss1: 0.379 | Loss2: 0.36232 | Loss3: 0.21911 | Loss_concat: 0.54227 |
Iteration 69 | train_acc = 82.95455 | train_loss = 1.73474 | Loss1: 0.399 | Loss2: 0.37362 | Loss3: 0.26014 | Loss_concat: 0.70208 |
Iteration 70 | train_acc = 85.51136 | train_loss = 1.56236 | Loss1: 0.370 | Loss2: 0.35061 | Loss3: 0.25438 | Loss_concat: 0.58702 |
Iteration 71 | train_acc = 86.93182 | train_loss = 1.48133 | Loss1: 0.385 | Loss2: 0.35513 | Loss3: 0.23282 | Loss_concat: 0.50817 |
Iteration 72 | train_acc = 84.37500 | train_loss = 1.56552 | Loss1: 0.389 | Loss2: 0.37057 | Loss3: 0.23005 | Loss_concat: 0.57608 |
Iteration 73 | train_acc = 84.65909 | train_loss = 1.50376 | Loss1: 0.384 | Loss2: 0.34976 | Loss3: 0.23426 | Loss_concat: 0.53540 |
Iteration 74 | train_acc = 85.51136 | train_loss = 1.54745 | Loss1: 0.393 | Loss2: 0.35573 | Loss3: 0.22696 | Loss_concat: 0.57206 |
Iteration 75 | train_acc = 84.37500 | train_loss = 1.50865 | Loss1: 0.376 | Loss2: 0.34541 | Loss3: 0.24229 | Loss_concat: 0.54483 |
Iteration 76 | train_acc = 85.79545 | train_loss = 1.52415 | Loss1: 0.372 | Loss2: 0.34514 | Loss3: 0.24652 | Loss_concat: 0.56007 |
Iteration 77 | train_acc = 82.95455 | train_loss = 1.54783 | Loss1: 0.383 | Loss2: 0.35903 | Loss3: 0.24295 | Loss_concat: 0.56285 |
Iteration 78 | train_acc = 86.93182 | train_loss = 1.44634 | Loss1: 0.379 | Loss2: 0.35754 | Loss3: 0.23105 | Loss_concat: 0.47856 |
Iteration 79 | train_acc = 86.07955 | train_loss = 1.46407 | Loss1: 0.375 | Loss2: 0.35763 | Loss3: 0.23527 | Loss_concat: 0.49638 |
Iteration 80 | train_acc = 84.09091 | train_loss = 1.43209 | Loss1: 0.377 | Loss2: 0.34666 | Loss3: 0.22107 | Loss_concat: 0.48756 |
Iteration 81 | train_acc = 83.52273 | train_loss = 1.48605 | Loss1: 0.367 | Loss2: 0.34672 | Loss3: 0.23994 | Loss_concat: 0.53225 |
Iteration 82 | train_acc = 85.79545 | train_loss = 1.45707 | Loss1: 0.371 | Loss2: 0.33921 | Loss3: 0.26625 | Loss_concat: 0.48064 |
Iteration 83 | train_acc = 85.51136 | train_loss = 1.50070 | Loss1: 0.385 | Loss2: 0.35176 | Loss3: 0.25134 | Loss_concat: 0.51270 |
Iteration 84 | train_acc = 85.79545 | train_loss = 1.40135 | Loss1: 0.368 | Loss2: 0.32968 | Loss3: 0.24070 | Loss_concat: 0.46304 |
Iteration 85 | train_acc = 84.65909 | train_loss = 1.45276 | Loss1: 0.374 | Loss2: 0.34401 | Loss3: 0.23823 | Loss_concat: 0.49617 |
Iteration 86 | train_acc = 84.65909 | train_loss = 1.40908 | Loss1: 0.371 | Loss2: 0.33463 | Loss3: 0.22936 | Loss_concat: 0.47407 |
Iteration 87 | train_acc = 84.37500 | train_loss = 1.43696 | Loss1: 0.381 | Loss2: 0.34041 | Loss3: 0.23602 | Loss_concat: 0.47943 |
Iteration 88 | train_acc = 84.94318 | train_loss = 1.41694 | Loss1: 0.379 | Loss2: 0.33645 | Loss3: 0.23127 | Loss_concat: 0.47007 |
Iteration 89 | train_acc = 88.06818 | train_loss = 1.38406 | Loss1: 0.356 | Loss2: 0.33396 | Loss3: 0.22966 | Loss_concat: 0.46471 |
Iteration 90 | train_acc = 86.07955 | train_loss = 1.38978 | Loss1: 0.369 | Loss2: 0.33437 | Loss3: 0.24178 | Loss_concat: 0.44505 |
Iteration 91 | train_acc = 87.21591 | train_loss = 1.31354 | Loss1: 0.370 | Loss2: 0.33254 | Loss3: 0.21395 | Loss_concat: 0.39699 |
Iteration 92 | train_acc = 87.21591 | train_loss = 1.36903 | Loss1: 0.363 | Loss2: 0.33760 | Loss3: 0.23518 | Loss_concat: 0.43309 |
Iteration 93 | train_acc = 85.79545 | train_loss = 1.37963 | Loss1: 0.377 | Loss2: 0.33129 | Loss3: 0.22792 | Loss_concat: 0.44310 |
Iteration 94 | train_acc = 85.79545 | train_loss = 1.42918 | Loss1: 0.373 | Loss2: 0.34146 | Loss3: 0.25863 | Loss_concat: 0.45596 |
Iteration 95 | train_acc = 85.79545 | train_loss = 1.39116 | Loss1: 0.359 | Loss2: 0.34347 | Loss3: 0.22741 | Loss_concat: 0.46121 |
Iteration 96 | train_acc = 86.64773 | train_loss = 1.37779 | Loss1: 0.366 | Loss2: 0.33681 | Loss3: 0.22871 | Loss_concat: 0.44670 |
Iteration 97 | train_acc = 87.78409 | train_loss = 1.33200 | Loss1: 0.355 | Loss2: 0.32606 | Loss3: 0.22792 | Loss_concat: 0.42308 |
Iteration 98 | train_acc = 84.94318 | train_loss = 1.39533 | Loss1: 0.365 | Loss2: 0.32606 | Loss3: 0.23815 | Loss_concat: 0.46578 |
Iteration 99 | train_acc = 86.64773 | train_loss = 1.34210 | Loss1: 0.368 | Loss2: 0.33329 | Loss3: 0.22541 | Loss_concat: 0.41579 |
